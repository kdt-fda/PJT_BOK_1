import pandas as pd
import kss
import gc
from ekonlpy.sentiment import MPCK
from tqdm import tqdm

def initialize_kss():
    print('ë¬¸ì¥ ë¶„ë¦¬ ì—”ì§„ ì„¤ì • ì¤‘...')
    try:
        kss.split_sentences('í…ŒìŠ¤íŠ¸', backend='mecab')
        print('Mecab ì—”ì§„ ë¡œë“œ ì™„ë£Œ')
        return 'mecab'
    except Exception as e:
        print(f'Mecab ë¡œë“œ ì‹¤íŒ¨({e}), ê¸°ë³¸ ì—”ì§„ìœ¼ë¡œ ì „í™˜')
        return 'keunmago'

def preprocess_news(input_file, output_file, batch_size=2000):
    # ì—”ì§„ ë° ë¶„ì„ê¸° ì´ˆê¸°í™”
    backend = initialize_kss()
    analyzer = MPCK()
    
    # ë°ì´í„° ë¡œë“œ
    print(f'ë°ì´í„° ì½ëŠ” ì¤‘: {input_file}')
    df = pd.read_csv(input_file, encoding='utf-8-sig')
    raw_data = list(zip(df['date'], df['cleansed_text']))
    
    final_data = []
    print(f'ë¶„ì„ ì‹œì‘, ì´ {len(raw_data)}ê±´')
    
    # ë£¨í”„ ì‹¤í–‰
    for i, (date, content) in enumerate(tqdm(raw_data, desc=f"Processing {input_file}")):
        if pd.isnull(content) or not str(content).strip():
            continue
        
        try:
            # ë¬¸ì¥ ë¶„ë¦¬
            sentences = kss.split_sentences(str(content), backend=backend)
            
            for sent in sentences:
                if len(sent) < 5: continue # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                
                # í† í°í™” ë° í’ˆì‚¬ í•„í„°ë§ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬, ë¶€ì •ì–´)
                tokens = analyzer.tokenize(sent)
                filtered = [t for t in tokens if '/' in t and 
                            t.split('/')[-1].startswith(('N', 'V', 'M'))]
                
                if filtered:
                    final_data.append([str(date), ",".join(filtered)])
            
            # ì¤‘ê°„ ì €ì¥
            if (i + 1) % batch_size == 0:
                save_header = True if (i + 1) == batch_size else False
                save_mode = 'w' if (i + 1) == batch_size else 'a'
                
                temp_df = pd.DataFrame(final_data, columns=['date', 'tokens'])
                temp_df.to_csv(output_file, index=False, encoding='utf-8-sig', 
                               mode=save_mode, header=save_header)
                
                final_data = []
                gc.collect()

        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê±´ë„ˆë›°ê¸°
            continue

    # ì”ì—¬ ë°ì´í„° ì €ì¥
    if final_data:
        temp_df = pd.DataFrame(final_data, columns=['date', 'tokens'])
        has_header = not pd.io.common.file_exists(output_file)
        temp_df.to_csv(output_file, index=False, encoding='utf-8-sig', 
                       mode='a', header=has_header)

    print(f"\nğŸ‰ {output_file} ì €ì¥ ì™„ë£Œ!")

# ì‹¤ì œ ì‹¤í–‰
if __name__ == "__main__":
    target_files = [
        ('chosun_news_cleansing.csv', 'chosun_news_tokenize.csv'),
        ('donga_news_cleansing.csv', 'donga_news_tokenize.csv'),
        ('khan_news_cleansing.csv', 'khan_news_tokenize.csv')
    ]
    
    for input, output in target_files:
        preprocess_news(input, output)
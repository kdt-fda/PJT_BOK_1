{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07550d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "# 조선일보 검색 4000개 제한으로 기간 분할\n",
    "date_ranges = [\n",
    "    ('20050501', '20120430'),\n",
    "    ('20120501', '20190430'),\n",
    "    ('20190501', '20241231')\n",
    "]\n",
    "\n",
    "def crawl_news_chosun(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        res.encoding = 'utf-8'\n",
    "        \n",
    "        pattern = r'Fusion\\.globalContent\\s*=\\s*(\\{.*?\\});'\n",
    "        match = re.search(pattern, res.text)\n",
    "        \n",
    "        if match:\n",
    "            data = json.loads(match.group(1))\n",
    "            \n",
    "            title = data.get('headlines', {}).get('basic', '제목 없음').strip()\n",
    "            \n",
    "            date = data.get('display_date', '날짜 정보 없음')\n",
    "            \n",
    "            elements = data.get('content_elements', [])\n",
    "            paragraphs = [re.sub(r'<[^>]*>', '', el.get('content', '')) \n",
    "                          for el in elements if el.get('type') == 'text']\n",
    "            content = \"\\n\".join(paragraphs)\n",
    "            \n",
    "            return {\n",
    "                'date': date,\n",
    "                'title': title,\n",
    "                'content': content\n",
    "            }\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_urls = []\n",
    "    \n",
    "    # 1. 검색 API를 통해 모든 기간의 기사 URL 먼저 수집\n",
    "    print(\">>> 1단계: URL 목록 수집 시작\")\n",
    "    for s_date, e_date in date_ranges:\n",
    "        print(f\"기간 {s_date} ~ {e_date} 처리 중...\")\n",
    "        for page in range(400):\n",
    "            search_url = f'https://search-gateway.chosun.com/nsearch?query=%ED%95%9C%EA%B5%AD%EC%9D%80%ED%96%89%20%EA%B8%88%EB%A6%AC&page={page}&size=10&sort=2&r=direct&s={s_date}&e={e_date}'\n",
    "            try:\n",
    "                res = requests.get(search_url, headers=headers, timeout=10)\n",
    "                search_data = json.loads(res.text)\n",
    "                items = search_data.get('content_elements', [])\n",
    "                if not items: break\n",
    "                \n",
    "                for item in items:\n",
    "                    if item.get('site_url'):\n",
    "                        all_urls.append(item['site_url'])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    print(f\"\\n>>> 총 {len(all_urls)}개의 URL 확보. 2단계 본문 수집 시작 (Thread: 10)\")\n",
    "\n",
    "    # 2. 멀티스레딩 활용\n",
    "    final_docs = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(crawl_news_chosun, url): url for url in all_urls}\n",
    "        \n",
    "        for i, future in enumerate(as_completed(future_to_url)):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                final_docs.append(result)\n",
    "            \n",
    "            # 진행 상황\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"[{i + 1}/{len(all_urls)}] 데이터 수집 완료...\")\n",
    "\n",
    "    # 3. 결과 저장\n",
    "    print(f\"\\n>>> 최종 수집된 기사 수: {len(final_docs)}개\")\n",
    "\n",
    "\n",
    "    with open('news_corpus_for_thesis.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_docs, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'news_corpus_for_thesis.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# [날짜, 제목 + 본문]\n",
    "article_list = []\n",
    "\n",
    "for item in data:\n",
    "    date = item.get('date', '')\n",
    "    title = item.get('title', '')\n",
    "    content = item.get('content', '')\n",
    "    \n",
    "    full_text = f\"{title}\\n\\n{content}\"\n",
    "    \n",
    "    article_list.append([date, full_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2095ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file_path = 'news_chosun.csv'\n",
    "\n",
    "with open(csv_file_path, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    writer.writerow(['date', 'full_text'])\n",
    "    \n",
    "    writer.writerows(article_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

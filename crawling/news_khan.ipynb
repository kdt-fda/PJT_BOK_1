{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "def crawl_news_khan(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        res.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        date_element = soup.find('div', {'class': 'date'}) or soup.find('span', {'class': 'date'})\n",
    "        if date_element:\n",
    "            date_raw = date_element.get_text(strip=True)\n",
    "            date_match = re.search(r'\\d{4}[.\\-]\\d{2}[.\\-]\\d{2}', date_raw)\n",
    "            clean_date = date_match.group().replace(\".\", \"-\") if date_match else \"날짜없음\"\n",
    "        else:\n",
    "            clean_date = \"날짜없음\"\n",
    "\n",
    "        title = \"제목없음\"\n",
    "        title_tags = soup.find_all('h1')\n",
    "        if len(title_tags) > 1:\n",
    "            title = title_tags[1].get_text(strip=True)\n",
    "        elif len(title_tags) == 1:\n",
    "            title = title_tags[0].get_text(strip=True)\n",
    "\n",
    "        content_elements = soup.find_all('p', {'class': 'content_text'})\n",
    "        if content_elements:\n",
    "            full_content = '\\n'.join([el.get_text(strip=True) for el in content_elements])\n",
    "        else:\n",
    "            article_body = soup.find('div', {'class': 'article_txt'}) or \\\n",
    "                           soup.find('article', {'id': 'articleBody'}) or \\\n",
    "                           soup.find('div', {'class': 'art_body'})\n",
    "            if article_body:\n",
    "                for tag in article_body.find_all(['script', 'style', 'figure', 'div', 'iframe']):\n",
    "                    tag.decompose()\n",
    "                full_content = article_body.get_text(strip=True)\n",
    "            else:\n",
    "                full_content = \"\"\n",
    "\n",
    "        stops = [\"뱅크-아이\", \"무단전재\", \"기자 =\", \"기자=\", \"@khan.co.kr\", \"ⓒ\", \"제공=\", \"출처=\"]\n",
    "        for stop in stops:\n",
    "            if stop in full_content:\n",
    "                full_content = full_content.split(stop)[0]\n",
    "\n",
    "        full_content = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', full_content)\n",
    "        full_content = re.sub(r'http[s]?://\\S+', '', full_content)\n",
    "        full_content = full_content.strip()\n",
    "\n",
    "        # [날짜, 제목+본문]\n",
    "        if full_content:\n",
    "            return [clean_date, f\"{title}\\n\\n{full_content}\"]\n",
    "        return None\n",
    "\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_urls = []\n",
    "    pages = 393\n",
    "\n",
    "    # 1. URL 목록 수집\n",
    "    print(\">>> 1단계: 경향신문 URL 목록 수집 시작\")\n",
    "    for page in range(1, pages + 1):\n",
    "        base_url = f'https://search.khan.co.kr/?q=%ED%95%9C%EA%B5%AD%EC%9D%80%ED%96%89+%EA%B8%88%EB%A6%AC&media=khan&page={page}&section=0&term=5&startDate=2015-01-01&endDate=2024-12-31&sort=2'\n",
    "        try:\n",
    "            res = requests.get(base_url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "            datas = soup.find_all('a', {'ep_event_area': '검색결과_기사목록'})\n",
    "            \n",
    "            for data in datas:\n",
    "                url = data.get('href')\n",
    "                if url:\n",
    "                    all_urls.append(url)\n",
    "            \n",
    "            if page % 50 == 0:\n",
    "                print(f\"[{page}/{pages}] 페이지 URL 수집 완료 (누적: {len(all_urls)}개)\")\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n>>> 총 {len(all_urls)}개의 URL 확보. 2단계 본문 수집 시작 (Thread: 10)\")\n",
    "\n",
    "    # 2. 멀티스레딩 활용\n",
    "    final_docs = []\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_url = {executor.submit(crawl_news_khan, url): url for url in all_urls}\n",
    "        \n",
    "        for i, future in enumerate(as_completed(future_to_url)):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                final_docs.append(result)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"[{i + 1}/{len(all_urls)}] 데이터 추출 완료...\")\n",
    "\n",
    "    # 3. CSV 파일 저장\n",
    "    csv_file_path = 'news_khan_enhanced.csv'\n",
    "    with open(csv_file_path, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['date', 'full_text'])\n",
    "        writer.writerows(final_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9eb747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(csv_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
